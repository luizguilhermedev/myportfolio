{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMc12xVRBRXjMR7e5n6r1+z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luizguilhermedev/myportfolio/blob/main/lunar_lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep reinforcement Learning agent ü§ñ üß†\n",
        "\n",
        "A lunar lander that will learn to land correctly on the Moon üåù\n",
        "\n",
        "### Environment\n",
        "\n",
        "* We're going to use `LunarLander-v2`\n",
        "* `Stable-Baselines3`for the library\n",
        "\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "* `gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ\n",
        "\n",
        "* `stable-baselines3[extra]`: The deep reinforcement learning library.\n",
        "\n",
        "* `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n",
        "\n",
        "\n",
        "## Problem\n",
        "\n",
        "**Being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine.**\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "Obtain 200+ with our Agent\n"
      ],
      "metadata": {
        "id": "BzzRoUWYdFjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install swig cmake"
      ],
      "metadata": {
        "id": "OCUvkEnuee0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ],
      "metadata": {
        "id": "dTbCPfkeejW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "iLu6tRGYepD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will force the runtime to crash, so we'll need to connect again and run the code starting from here\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(),9)"
      ],
      "metadata": {
        "id": "5Qr5c8tle21p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "q7GUnqlbfPvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "Hy_Q5cyWfqN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gymnasium ü§ñ üèã\n",
        "\n",
        "This library contains our environment\n",
        "\n",
        "Gymnasium library provides two things:\n",
        "\n",
        "* An interface that allows us to create **RL environments**\n",
        "* A collection of environments(such as gym-control, atari, box2D...)\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Agent receives state (S0) from the environment\n",
        "* Based on that stat (S0), the Agent taken an action (A0)\n",
        "* Environment transitions to new state (S1)\n",
        "* Environment gives some reward(R1)\n",
        "\n",
        "Gymnasium:\n",
        "\n",
        "* Create our environment using `gymnasium.make()`\n",
        "* Reset the environment to its initial state with `observation = env.reset()`\n",
        "* Get an action using our model\n",
        "* `env.step(action)`"
      ],
      "metadata": {
        "id": "vT6jo2eOf_ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# Creating environment called LunarLander-v2\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Reset this environment\n",
        "\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get next_state, reward, terminated, truncated and info\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  # if the game is terminated (landed, crashed) or truncated (timeout)\n",
        "\n",
        "  if terminated or truncated:\n",
        "    # reset the environment\n",
        "    print(\"Environment is reset\")\n",
        "    observation, info = env.reset()\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "lUpLjEGogjy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look what our Environment looks like:"
      ],
      "metadata": {
        "id": "14GkY5U0fGMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "Ky7aGMPWfSdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "DCnXhDSnfX1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorized Environment\n",
        "\n",
        "Stacking multiple independent environments into a single environment"
      ],
      "metadata": {
        "id": "hZDz0JUAgD7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ],
      "metadata": {
        "id": "_y84a1w0glx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our Model ü§ñ\n",
        "\n",
        "\n",
        "* Deep RL library: Stable Baselines3(SB3)\n",
        "* SB3 is  set of reliable implementations of reinforcement learning algotithms in PyTorch.\n",
        "\n",
        "SB3 basic steps:\n",
        "\n",
        "* 1. Create an environment\n",
        "* 2. Define the model we want to use `model = PPO(\"MlpPolicy)`\n",
        "* 3. Train the agent with `model.learn`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q8oELoZRgtFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Instantiate the agent\n",
        "\n",
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "\n",
        "# model.learn(total_timesteps=int(2e5))"
      ],
      "metadata": {
        "id": "ZEW6HfQGiRLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning to accelerate the trainig\n",
        "\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "8cpPHhOIi_W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train it for 2,000,000 timesteps\n",
        "model.learn(total_timesteps=200000, progress_bar=True)\n",
        "\n",
        "# and let's save the model\n",
        "\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "_bJrOr4rjfNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our Agent ü§ñ ü¶æ\n",
        "\n",
        "* Wrap the environment in a Monitor\n",
        "* SB3 provides a method to do that: `evaluate_policy`\n"
      ],
      "metadata": {
        "id": "7FFEF64VkaEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the agent\n",
        "# NOTE: If you use wrappers with your environment that modify rewards,\n",
        "#       this will be reflected here. To evaluate with original rewards,\n",
        "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
        "\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f'Mean Reward = {mean_reward:.2f} +/- {std_reward}')"
      ],
      "metadata": {
        "id": "rbdlNjWTla5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WELL DONE!\n",
        "\n",
        "Our agent has trained 1500000 times and got 227 points\n",
        "\n",
        "**Note:** It's possible to improve our model even more, trying different hyperparameters"
      ],
      "metadata": {
        "id": "pG6uHP8AKnGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "dkRWEXWdC2VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from huggingface_sb3 import package_to_hub\n",
        "\n",
        "# PLACE the variables you've just defined two cells above\n",
        "# Define the name of the environment\n",
        "env_id = \"LunarLander-v2\"\n",
        "\n",
        "# TODO: Define the model architecture we used\n",
        "model_architecture = \"PPO\"\n",
        "\n",
        "## Define a repo_id\n",
        "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
        "## CHANGE WITH YOUR REPO ID\n",
        "repo_id = \"luizguilherme/ppo-LunarLander-v2\"\n",
        "\n",
        "## Define the commit message\n",
        "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
        "\n",
        "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "# PLACE the package_to_hub function you've just filled here\n",
        "package_to_hub(model=model, # Our trained model\n",
        "               model_name=model_name, # The name of our trained model\n",
        "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
        "               env_id=env_id, # Name of the environment\n",
        "               eval_env=eval_env, # Evaluation Environment\n",
        "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
        "               commit_message=commit_message)\n"
      ],
      "metadata": {
        "id": "DZq0e2exHKa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "id": "-PgzYexnJAzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_sb3 import load_from_hub\n",
        "repo_id = \"luizguilherme/ppo-LunarLander-v2\" # The repo_id\n",
        "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
        "\n",
        "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
        "# But Python 3.6, 3.7 use protocol 4\n",
        "# In order to get compatibility we need to:\n",
        "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
        "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
        "custom_objects = {\n",
        "            \"learning_rate\": 0.0,\n",
        "            \"lr_schedule\": lambda _: 0.0,\n",
        "            \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ],
      "metadata": {
        "id": "UWh5CTP3JF2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "kt6lPiokJJ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMtxOVqRJNps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}